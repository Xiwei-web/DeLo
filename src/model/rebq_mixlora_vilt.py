import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict
import math

from transformers import ViltModel, ViltConfig
# å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„å®˜æ–¹åŸºç±»
from transformers.models.vilt.modeling_vilt import (
    ViltLayer, 
    ViltAttention, 
    ViltSelfAttention
)

from .mixlora_layer import MixLoRALayer 
from loguru import logger
# åœ¨MixLoRAWrapperä¸­æ·»åŠ ä¸€ä¸ªset_active_taskæ–¹æ³•
# class MixLoRAWrapper(nn.Module):
#     def __init__(self, linear_layer: nn.Linear, model_cfg):
#         super().__init__()
#         self.frozen_layer = linear_layer
#         num_tasks = model_cfg.NUM_TASKS
#         in_features, out_features = linear_layer.in_features, linear_layer.out_features
        
#         # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¼•ç”¨çš„æ˜¯é‡æ„åçš„MixLoRALayer
#         self.vision_adapter = MixLoRALayer(in_features, out_features, r=model_cfg.LORA_R, E=model_cfg.LORA_E, num_tasks=num_tasks, alpha=model_cfg.LORA_ALPHA)
#         self.text_adapter = MixLoRALayer(in_features, out_features, r=model_cfg.LORA_R, E=model_cfg.LORA_E, num_tasks=num_tasks, alpha=model_cfg.LORA_ALPHA)

#     def set_active_task(self, task_id: int):
#         self.vision_adapter.set_active_task(task_id)
#         self.text_adapter.set_active_task(task_id)

#     def forward(self, hidden_states: torch.Tensor, *args, **kwargs):
#         # æ­¥éª¤1ï¼šå…ˆç”¨å†»ç»“å±‚è®¡ç®—åŸºç¡€è¾“å‡º
#         frozen_output = self.frozen_layer(hidden_states)
#         base_output = frozen_output[0] if isinstance(frozen_output, tuple) else frozen_output

#         # æ­¥éª¤2ï¼šè·å–æŸ¥è¯¢ä¿¡å·
#         query_signals = getattr(hidden_states, 'query_signals', None)

#         # å…³é”®ä¿®å¤ï¼šåªæœ‰åœ¨query_signalså®Œå…¨ä¸å­˜åœ¨æ—¶æ‰çŸ­è·¯è¿”å›
#         # åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œæ— è®ºæ˜¯è®­ç»ƒè¿˜æ˜¯è¯„ä¼°ï¼Œquery_signalséƒ½åº”è¯¥è¢«åˆ›å»ºå’Œä¼ é€’
#         if query_signals is None:
#             return frozen_output

#         # æ­¥éª¤3ï¼šè®¡ç®—LoRAå¢é‡ (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
#         q_v, q_t, is_text_missing = query_signals['vision'], query_signals['text'], query_signals['is_text_missing']
        
#         vision_lora_delta = self.vision_adapter(hidden_states, q_v)
        
#         # ç¡®ä¿q_vå’Œq_tçš„å½¢çŠ¶ä¸€è‡´ï¼Œä»¥ç”¨äºtorch.where
#         if q_v.dim() == 1 and q_t.dim() > 1:
#             q_v = q_v.unsqueeze(0).expand_as(q_t)

#         text_query_signal = torch.where(is_text_missing.unsqueeze(-1).expand_as(q_t), q_v, q_t)
#         text_lora_delta = self.text_adapter(hidden_states, text_query_signal)

#         # æ­¥éª¤4ï¼šåº”ç”¨å¢é‡
#         final_output = base_output + vision_lora_delta + text_lora_delta

#         if isinstance(frozen_output, tuple):
#             return (final_output,) + frozen_output[1:]
#         return final_output
class MixLoRAWrapper(nn.Module):
    def __init__(self, linear_layer: nn.Linear, model_cfg, layer_name: str = "unknown"):
        super().__init__()
        self.frozen_layer = linear_layer
        self.layer_name = layer_name  # ğŸ”§ ä¿®å¤5: æ·»åŠ å±‚æ ‡è¯†
        num_tasks = model_cfg.NUM_TASKS
        in_features, out_features = linear_layer.in_features, linear_layer.out_features
        
        # åˆ›å»ºåŒæ¨¡æ€é€‚é…å™¨
        self.vision_adapter = MixLoRALayer(
            in_features, out_features, 
            r=model_cfg.LORA_R, E=model_cfg.LORA_E, 
            num_tasks=num_tasks, alpha=model_cfg.LORA_ALPHA
        )
        self.text_adapter = MixLoRALayer(
            in_features, out_features, 
            r=model_cfg.LORA_R, E=model_cfg.LORA_E, 
            num_tasks=num_tasks, alpha=model_cfg.LORA_ALPHA
        )
        
        # ğŸ”§ ä¿®å¤6: è®¾ç½®è°ƒè¯•åç§°
        self.vision_adapter.set_debug_name(f"{layer_name}_vision")
        self.text_adapter.set_debug_name(f"{layer_name}_text")

    def set_active_task(self, task_id: int):
        """ğŸ”§ ä¼˜åŒ–4: åªåœ¨ç¬¬0å±‚æ‰“å°è¯¦ç»†ä¿¡æ¯"""
        if "layer_0" in self.layer_name:
            logger.info(f"[{self.layer_name}] åˆ‡æ¢åˆ°ä»»åŠ¡{task_id}")
        self.vision_adapter.set_active_task(task_id)
        self.text_adapter.set_active_task(task_id)

    def forward(self, hidden_states: torch.Tensor, *args, **kwargs):
        # å†»ç»“å±‚è®¡ç®—
        frozen_output = self.frozen_layer(hidden_states)
        base_output = frozen_output[0] if isinstance(frozen_output, tuple) else frozen_output

        # è·å–æŸ¥è¯¢ä¿¡å·
        query_signals = getattr(hidden_states, 'query_signals', None)
        if query_signals is None:
            return frozen_output

        # è®¡ç®—LoRAå¢é‡
        q_v, q_t, is_text_missing = query_signals['vision'], query_signals['text'], query_signals['is_text_missing']
        
        vision_lora_delta = self.vision_adapter(hidden_states, q_v)
        
        # å¤„ç†æ–‡æœ¬ç¼ºå¤±æƒ…å†µ
        if q_v.dim() == 1 and q_t.dim() > 1:
            q_v = q_v.unsqueeze(0).expand_as(q_t)

        text_query_signal = torch.where(is_text_missing.unsqueeze(-1).expand_as(q_t), q_v, q_t)
        text_lora_delta = self.text_adapter(hidden_states, text_query_signal)

        # åº”ç”¨å¢é‡
        final_output = base_output + vision_lora_delta + text_lora_delta

        if isinstance(frozen_output, tuple):
            return (final_output,) + frozen_output[1:]
        return final_output


class MixLoRAViltSelfAttention(ViltSelfAttention):
    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):
        query_layer = self.transpose_for_scores(self.query(hidden_states))
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / math.sqrt(self.attention_head_size)
        if attention_mask is not None: attention_scores = attention_scores + attention_mask
        attention_probs = nn.Softmax(dim=-1)(attention_scores)
        attention_probs = self.dropout(attention_probs)
        if head_mask is not None: attention_probs = attention_probs * head_mask
        context_layer = torch.matmul(attention_probs, value_layer).permute(0, 2, 1, 3).contiguous()
        new_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_shape)
        return (context_layer, attention_probs) if output_attentions else (context_layer,)

class MixLoRAViltAttention(ViltAttention):
    def __init__(self, config):
        super().__init__(config)
        self.attention = MixLoRAViltSelfAttention(config)
    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):
        self_outputs = self.attention(hidden_states, attention_mask, head_mask, output_attentions)
        attention_output = self.output(self_outputs[0], hidden_states)
        return (attention_output,) + self_outputs[1:]

class MixLoRAViltLayer(ViltLayer):
    def __init__(self, config):
        super().__init__(config)
        self.attention = MixLoRAViltAttention(config)
    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):
        attention_outputs = self.attention(self.layernorm_before(hidden_states), attention_mask, head_mask, output_attentions)
        hidden_states = attention_outputs[0] + hidden_states
        layer_output = self.output(self.intermediate(self.layernorm_after(hidden_states)), hidden_states)
        return (layer_output,) + attention_outputs[1:]


# class RebQMixLoRAVilt(nn.Module):
#     def __init__(self, model_cfg):
#         super().__init__()
#         self.model_cfg = model_cfg
        
#         # 1. é¦–å…ˆï¼ŒåªåŠ è½½åŸå§‹çš„é…ç½®å¯¹è±¡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
#         config = ViltConfig.from_pretrained("dandelin/vilt-b32-mlm")
        
#         # 2. è·å–æˆ‘ä»¬éœ€è¦çš„æœ€å¤§é•¿åº¦
#         max_len_config = model_cfg.get("MAX_LENGTH", config.max_position_embeddings)

#         # 3. æ£€æŸ¥å¹¶ç›´æ¥ä¿®æ”¹è¿™ä¸ªé…ç½®å¯¹è±¡
#         if max_len_config > config.max_position_embeddings:
#             print(f"\n[INFO] Modifying config for max_position_embeddings: {config.max_position_embeddings} -> {max_len_config}\n")
#             config.max_position_embeddings = max_len_config
        
#         # 4. ä½¿ç”¨æˆ‘ä»¬ä¿®æ”¹è¿‡çš„ã€å°ºå¯¸æ­£ç¡®çš„configï¼Œå»åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
#         #    ignore_mismatched_sizes=True å…è®¸åº“æ™ºèƒ½åœ°å¤„ç†å°ºå¯¸ä¸ç¬¦çš„æƒé‡
#         self.vilt = ViltModel.from_pretrained(
#             "dandelin/vilt-b32-mlm",
#             config=config,
#             ignore_mismatched_sizes=True # å…³é”®ï¼
#         )
#         # ==============================================================================

#         # LoRAæ³¨å…¥é€»è¾‘ (ä¿æŒä¸å˜)
#         for layer in self.vilt.encoder.layer:
#             layer.attention.attention.query = MixLoRAWrapper(layer.attention.attention.query, model_cfg)
#             layer.attention.attention.key = MixLoRAWrapper(layer.attention.attention.key, model_cfg)
#             layer.attention.attention.value = MixLoRAWrapper(layer.attention.attention.value, model_cfg)
    

#     def set_active_task(self, task_id: int):
#         for layer in self.vilt.encoder.layer:
#             layer.attention.attention.query.set_active_task(task_id)
#             layer.attention.attention.key.set_active_task(task_id)
#             layer.attention.attention.value.set_active_task(task_id)
class RebQMixLoRAVilt(nn.Module):
    def __init__(self, model_cfg):
        super().__init__()
        self.model_cfg = model_cfg
        
        # åŠ è½½é…ç½®å’Œæ¨¡å‹...
        config = ViltConfig.from_pretrained("dandelin/vilt-b32-mlm")
        max_len_config = model_cfg.get("MAX_LENGTH", config.max_position_embeddings)

        if max_len_config > config.max_position_embeddings:
            print(f"\n[INFO] Modifying config for max_position_embeddings: {config.max_position_embeddings} -> {max_len_config}\n")
            config.max_position_embeddings = max_len_config
        
        self.vilt = ViltModel.from_pretrained(
            "dandelin/vilt-b32-mlm",
            config=config,
            ignore_mismatched_sizes=True
        )

        # ğŸ”§ ä¼˜åŒ–5: ç®€åŒ–LoRAæ³¨å…¥æ—¥å¿—
        total_layers = len(self.vilt.encoder.layer)
        logger.info(f"å¼€å§‹ä¸º {total_layers} å±‚æ³¨å…¥ MixLoRA (query, key, value)...")
        
        injection_success = 0
        for layer_idx, layer in enumerate(self.vilt.encoder.layer):
            layer_name = f"layer_{layer_idx}"
            
            try:
                # Queryæ³¨å…¥
                layer.attention.attention.query = MixLoRAWrapper(
                    layer.attention.attention.query, model_cfg, f"{layer_name}_query"
                )
                
                # Keyæ³¨å…¥
                layer.attention.attention.key = MixLoRAWrapper(
                    layer.attention.attention.key, model_cfg, f"{layer_name}_key"
                )
                
                # Valueæ³¨å…¥
                layer.attention.attention.value = MixLoRAWrapper(
                    layer.attention.attention.value, model_cfg, f"{layer_name}_value"
                )
                
                injection_success += 1
                
                # ğŸ”§ ä¼˜åŒ–6: åªä¸ºæ¯3å±‚æ‰“å°ä¸€æ¬¡è¿›åº¦
                if (layer_idx + 1) % 3 == 0 or layer_idx == total_layers - 1:
                    logger.info(f"è¿›åº¦: {layer_idx + 1}/{total_layers} å±‚å®Œæˆ")
                    
            except Exception as e:
                logger.error(f"âŒ Layer {layer_idx} æ³¨å…¥å¤±è´¥: {e}")
        
        logger.info(f"âœ… MixLoRAæ³¨å…¥å®Œæˆ: {injection_success}/{total_layers} å±‚æˆåŠŸ")
        
        # å¿«é€ŸéªŒè¯ - åªæ£€æŸ¥å…³é”®ç»„ä»¶
        self._quick_verify()

    def _quick_verify(self):
        """ğŸ”§ ä¼˜åŒ–7: å¿«é€ŸéªŒè¯ï¼Œåªæ£€æŸ¥ç¬¬0å±‚å’Œæœ€åä¸€å±‚"""
        test_layers = [0, len(self.vilt.encoder.layer) - 1]
        success_count = 0
        
        for layer_idx in test_layers:
            layer = self.vilt.encoder.layer[layer_idx]
            if (isinstance(layer.attention.attention.query, MixLoRAWrapper) and
                isinstance(layer.attention.attention.key, MixLoRAWrapper) and
                isinstance(layer.attention.attention.value, MixLoRAWrapper)):
                success_count += 1
            else:
                logger.error(f"âŒ Layer {layer_idx} éªŒè¯å¤±è´¥")
        
        if success_count == len(test_layers):
            logger.info(f"âœ… LoRAæ³¨å…¥éªŒè¯é€šè¿‡ (æ£€æŸ¥äº†ç¬¬0å±‚å’Œæœ€åä¸€å±‚)")
        else:
            raise RuntimeError(f"LoRAæ³¨å…¥éªŒè¯å¤±è´¥!")


    def set_active_task(self, task_id: int):
        """ğŸ”§ ä¿®å¤1: ä¿®å¤ä»»åŠ¡åˆ‡æ¢ä¸­çš„é”™è¯¯"""
        logger.info(f"å…¨å±€åˆ‡æ¢åˆ°ä»»åŠ¡ {task_id}")
        
        switch_errors = 0
        total_layers = len(self.vilt.encoder.layer)
        
        for layer_idx, layer in enumerate(self.vilt.encoder.layer):
            try:
                layer.attention.attention.query.set_active_task(task_id)
                layer.attention.attention.key.set_active_task(task_id)
                layer.attention.attention.value.set_active_task(task_id)
            except Exception as e:
                switch_errors += 1
                if switch_errors <= 3:  # åªæŠ¥å‘Šå‰3ä¸ªé”™è¯¯
                    logger.error(f"Layer {layer_idx} ä»»åŠ¡åˆ‡æ¢å¤±è´¥: {e}")
                    # ğŸ”§ æ·»åŠ è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        if switch_errors == 0:
            logger.info(f"âœ… æ‰€æœ‰ {total_layers} å±‚å·²åˆ‡æ¢åˆ°ä»»åŠ¡ {task_id}")
        else:
            logger.error(f"âŒ {switch_errors} å±‚ä»»åŠ¡åˆ‡æ¢å¤±è´¥")

    # ==================== æ ¸å¿ƒä¿®å¤ï¼šæ˜ç¡®çš„å‡½æ•°ç­¾åæ¥æ¥æ”¶æ‰€æœ‰å‚æ•° ====================
    def forward(self, 
            input_ids, 
            pixel_values, 
            attention_mask=None, 
            token_type_ids=None,
            pixel_mask=None,
            inputs_embeds=None,
            image_embeds=None,
            force_task_id: int = None):

        # æ­¥éª¤ 1: å°†æ‰€æœ‰å°†è¦ä¼ é€’ç»™ embedding å±‚çš„å‚æ•°æ‰“åŒ…åˆ°ä¸€ä¸ªå­—å…¸ä¸­
        embedding_args = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "token_type_ids": token_type_ids,
            "pixel_values": pixel_values,
            "pixel_mask": pixel_mask,
            "inputs_embeds": inputs_embeds,
            "image_embeds": image_embeds,
        }
    
        # embedding_output, updated_attention_mask = self.vilt.embeddings(...)
        try:
            # æ­¥éª¤ 3: ä½¿ç”¨å­—å…¸è§£åŒ… `**` çš„æ–¹å¼è¿›è¡Œè°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç¨³å¥çš„å…³é”®å­—å‚æ•°ä¼ é€’æ–¹å¼
            embedding_output, updated_attention_mask = self.vilt.embeddings(**embedding_args)
        
        except TypeError as e:
            # æ­¥éª¤ 4: å¦‚æœä¾ç„¶æŠ¥é”™ï¼Œæˆ‘ä»¬å°†æ•è·å®ƒï¼Œå¹¶æ‰“å°å‡ºæ›´è¯¦ç»†çš„æ±‚æ•‘ä¿¡æ¯
            logger.critical("!!! è°ƒç”¨ self.vilt.embeddings æ—¶æ•è·åˆ° TypeError !!!")
            logger.critical(f"é”™è¯¯ä¿¡æ¯: {e}")
            # å†æ¬¡æ‰“å°å‚æ•°ä¿¡æ¯ï¼Œä»¥é˜²ä¸‡ä¸€
            for key, value in embedding_args.items():
                logger.critical(f"å´©æºƒæ—¶å‚æ•°è¯¦æƒ… -> {key}: {type(value)}")
            raise e # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ç¨‹åºåœæ­¢
        
        hidden_states = embedding_output
        extended_attention_mask = self.vilt.get_extended_attention_mask(updated_attention_mask, input_ids.shape)
        text_len = input_ids.shape[1]
        is_text_missing = (input_ids[:, 1:] == 0).all(dim=1)

        if force_task_id is not None:
            hidden_states.force_task_id = force_task_id
        
        # final_query_signals åˆå§‹åŒ–
        final_query_signals = {}
        for layer_module in self.vilt.encoder.layer:
            q_v = hidden_states[:, text_len:, :].mean(dim=1)
            q_t = hidden_states[:, 1:text_len, :].mean(dim=1)
            q_t[is_text_missing] = 0.0
            
            hidden_states.query_signals = {'vision': q_v, 'text': q_t, 'is_text_missing': is_text_missing}
            layer_outputs = layer_module(hidden_states, extended_attention_mask)
            hidden_states = layer_outputs[0]
            
            if hasattr(hidden_states, 'query_signals'):
                del hidden_states.query_signals
            
            # =================== æ ¸å¿ƒä¿®æ”¹ç‚¹ ===================
            # æ— è®ºæ˜¯å¦åœ¨è®­ç»ƒï¼Œæˆ‘ä»¬éƒ½è®°å½•æœ€åä¸€å±‚çš„query_signals
            # è¿™å¯¹äºæŸå¤±è®¡ç®—å’Œè°ƒè¯•éƒ½æœ‰å¥½å¤„
            if layer_module == self.vilt.encoder.layer[-1]:
                final_query_signals = {'vision': q_v, 'text': q_t}
            # ===============================================

        sequence_output = self.vilt.layernorm(hidden_states)
        
        if hasattr(hidden_states, 'force_task_id'):
            del hidden_states.force_task_id

        final_output = {
            "last_hidden_state": sequence_output,
            "pooler_output": self.vilt.pooler(sequence_output),
        }
    
        # æ— è®ºè®­ç»ƒè¿˜æ˜¯è¯„ä¼°ï¼Œéƒ½å°†æœ€åä¸€å±‚çš„ä¿¡å·è¿”å›
        final_output["query_signals"] = final_query_signals
            
        return final_output