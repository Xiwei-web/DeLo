# # æ–‡ä»¶: src/model/mixlora_layer.py (æœ€ç»ˆä¿®å¤ç‰ˆ - å›å½’ç‰©ç†åˆ†åŒº)

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from loguru import logger

# class MixLoRALayer(nn.Module):
#     def __init__(self, in_features: int, out_features: int, r: int, E: int, num_tasks: int, alpha: float = 1.0):
#         super().__init__()
#         self.in_features = in_features
#         self.out_features = out_features
#         self.r = r
#         self.E = E
#         self.num_tasks = num_tasks
#         self.alpha = alpha
#         self.current_task_id = 0

#         # ======================= æ ¸å¿ƒä¿®æ”¹ï¼šå›å½’ç‰©ç†åˆ†åŒºæ¶æ„ =======================
#         # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ã€ç‰©ç†éš”ç¦»çš„LoRA A/Bå› å­æ± 
#         # æ¯ä¸ªæ± çš„å¤§å°éƒ½æ˜¯ E
#         self.lora_A_pools = nn.ParameterList([
#             nn.Parameter(torch.randn(E, r, self.in_features)) for _ in range(num_tasks)
#         ])
#         self.lora_B_pools = nn.ParameterList([
#             nn.Parameter(torch.zeros(E, self.out_features, r)) for _ in range(num_tasks)
#         ])
        
#         # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„è·¯ç”±å™¨
#         self.ifs_router_A_list = nn.ModuleList([nn.Linear(self.in_features, E) for _ in range(num_tasks)])
#         self.ifs_router_B_list = nn.ModuleList([nn.Linear(self.in_features, E) for _ in range(num_tasks)])
#         self.cfs_router_B_weights_list = nn.ParameterList([
#             nn.Parameter(torch.randn(r, self.in_features, E)) for _ in range(num_tasks)
#         ])
#         # =======================================================================
        
#         # Query-Key è®°å¿†æ¨¡å—ä¿æŒä¸å˜
#         self.register_buffer('task_keys', torch.zeros(num_tasks, self.in_features))
#         self.ema_momentum = 0.99

#     # æ¢å¤ set_active_task æ–¹æ³•
#     def set_active_task(self, task_id: int):
#         self.current_task_id = task_id
#         for i in range(self.num_tasks):
#             is_active = (i == task_id)
#             # å†»ç»“/è§£å†»æ‰€æœ‰ä¸ä»»åŠ¡ç›¸å…³çš„å‚æ•°
#             self.lora_A_pools[i].requires_grad = is_active
#             self.lora_B_pools[i].requires_grad = is_active
#             self.cfs_router_B_weights_list[i].requires_grad = is_active
#             for param in self.ifs_router_A_list[i].parameters():
#                 param.requires_grad = is_active
#             for param in self.ifs_router_B_list[i].parameters():
#                 param.requires_grad = is_active

#     @torch.no_grad()
#     def _update_task_key(self, query_signal: torch.Tensor):
#         # æ­¤æ–¹æ³•ä¸å˜
#         batch_key = query_signal.mean(dim=0)
#         old_key = self.task_keys[self.current_task_id]
#         self.task_keys[self.current_task_id] = self.ema_momentum * old_key + (1 - self.ema_momentum) * batch_key

#     def forward(self, x: torch.Tensor, query_signal: torch.Tensor) -> torch.Tensor:
#         # forwardçš„é€»è¾‘ç°åœ¨å˜å¾—æ›´ç®€å•ï¼Œå› ä¸ºå®ƒä¾èµ–äº set_active_task
#         task_id_to_use = self.current_task_id
#         force_task_id = getattr(x, 'force_task_id', None)

#         if not self.training: # è¯„ä¼°æ¨¡å¼
#             if force_task_id is not None:
#                 task_id_to_use = force_task_id # å¼ºåˆ¶ä½¿ç”¨æŒ‡å®šçš„ä¸“å®¶
#             else: # è‡ªåŠ¨åŒ¹é…
#                 with torch.no_grad():
#                     task_keys_on_device = self.task_keys.to(query_signal.device)
#                     similarities = F.cosine_similarity(query_signal.unsqueeze(1), task_keys_on_device.unsqueeze(0), dim=-1)
#                     best_task_ids = torch.argmax(similarities, dim=1)
#                     task_id_to_use = torch.mode(best_task_ids).values.item()
#         elif self.training:
#              self._update_task_key(query_signal.detach())
        
#         # æ ¹æ®å†³ç­–å‡ºçš„ task_id_to_useï¼Œé€‰æ‹©æ¿€æ´»çš„ç»„ä»¶
#         active_A_pool = self.lora_A_pools[task_id_to_use]
#         active_B_pool = self.lora_B_pools[task_id_to_use]
#         active_ifs_router_A = self.ifs_router_A_list[task_id_to_use]
#         active_ifs_router_B = self.ifs_router_B_list[task_id_to_use]
#         active_cfs_router_B_weights = self.cfs_router_B_weights_list[task_id_to_use]

#         # åç»­çš„è·¯ç”±å’Œè®¡ç®—é€»è¾‘ä¸ä¹‹å‰å®Œå…¨ç›¸åŒ
#         g_A_scores = active_ifs_router_A(query_signal)
#         g_A_indices = torch.topk(g_A_scores, self.r, dim=-1).indices
#         lora_A = active_A_pool[g_A_indices]

#         g_B_scores_ifs = active_ifs_router_B(query_signal)
#         # æ³¨æ„ï¼šè¿™é‡Œçš„cfsæƒé‡ç»´åº¦å·²ç»é€‚é…äº†ç‰©ç†åˆ†åŒºæ¨¡å¼
#         g_B_scores_cfs = torch.einsum('bri,rie->be', lora_A.detach(), active_cfs_router_B_weights).sum(dim=1)
        
#         g_B_scores = g_B_scores_ifs + g_B_scores_cfs
#         g_B_indices = torch.topk(g_B_scores, self.r, dim=-1).indices
#         lora_B = active_B_pool[g_B_indices]
        
#         after_A = torch.einsum('bsi,bri->bsr', x, lora_A)
#         lora_delta = torch.einsum('bsr,bor->bso', after_A, lora_B)
        
#         return lora_delta * self.alpha


# æ–‡ä»¶1: src/model/mixlora_layer.py (ä¿®å¤ç‰ˆ)

import torch
import torch.nn as nn
import torch.nn.functional as F
from loguru import logger

class MixLoRALayer(nn.Module):
    def __init__(self, in_features: int, out_features: int, r: int, E: int, num_tasks: int, alpha: float = 1.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.r = r
        self.E = E
        self.num_tasks = num_tasks
        self.alpha = alpha
        self.current_task_id = 0

        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„ã€ç‰©ç†éš”ç¦»çš„LoRA A/Bå› å­æ± 
        self.lora_A_pools = nn.ParameterList([
            nn.Parameter(torch.randn(E, r, self.in_features)) for _ in range(num_tasks)
        ])
        self.lora_B_pools = nn.ParameterList([
            nn.Parameter(torch.zeros(E, self.out_features, r)) for _ in range(num_tasks)
        ])
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„è·¯ç”±å™¨
        self.ifs_router_A_list = nn.ModuleList([nn.Linear(self.in_features, E) for _ in range(num_tasks)])
        self.ifs_router_B_list = nn.ModuleList([nn.Linear(self.in_features, E) for _ in range(num_tasks)])
        self.cfs_router_B_weights_list = nn.ParameterList([
            nn.Parameter(torch.randn(r, self.in_features, E)) for _ in range(num_tasks)
        ])
        
        # Query-Key è®°å¿†æ¨¡å—
        self.register_buffer('task_keys', torch.zeros(num_tasks, self.in_features))
        self.ema_momentum = 0.99

        # ğŸ”§ ä¿®å¤1: æ·»åŠ è°ƒè¯•æ ‡è®°
        self.debug_name = "unnamed_layer"
        
        # ğŸ”§ ä¿®å¤2: åœ¨åˆå§‹åŒ–æ—¶å°±è®¾ç½®Task 0ä¸ºæ¿€æ´»çŠ¶æ€
        self.set_active_task(0)

    def set_debug_name(self, name: str):
        """è®¾ç½®è°ƒè¯•åç§°ï¼Œæ–¹ä¾¿è¿½è¸ªé—®é¢˜"""
        self.debug_name = name

    def set_active_task(self, task_id: int):
        """ğŸ”§ ä¿®å¤: ä¿®å¤ sum() å‡½æ•°çš„å‚æ•°é”™è¯¯"""
        old_task_id = self.current_task_id
        self.current_task_id = task_id
        
        for i in range(self.num_tasks):
            is_active = (i == task_id)
            
            # å†»ç»“/æ¿€æ´»LoRAå› å­æ± 
            self.lora_A_pools[i].requires_grad = is_active
            self.lora_B_pools[i].requires_grad = is_active
            self.cfs_router_B_weights_list[i].requires_grad = is_active
            
            # å†»ç»“/æ¿€æ´»è·¯ç”±å™¨å‚æ•°
            for param in self.ifs_router_A_list[i].parameters():
                param.requires_grad = is_active
            for param in self.ifs_router_B_list[i].parameters():
                param.requires_grad = is_active

        # ğŸ”§ ä¿®å¤: åªæœ‰åœ¨ç¬¬0å±‚çš„queryçš„visioné€‚é…å™¨æ—¶æ‰æ‰“å°è¯¦ç»†ä¿¡æ¯
        if self.debug_name == "layer_0_query_vision":
            # ğŸ”§ ä¿®å¤: å°†è®¡ç®—è¿‡ç¨‹åˆ†æ­¥è¿›è¡Œï¼Œé¿å… sum() å‡½æ•°çš„å‚æ•°é”™è¯¯
            
            # è®¡ç®—å½“å‰ä»»åŠ¡çš„æ¿€æ´»å‚æ•°æ•°é‡
            current_task_lora_params = (
                self.lora_A_pools[task_id].numel() + 
                self.lora_B_pools[task_id].numel() +
                self.cfs_router_B_weights_list[task_id].numel()
            )
            
            current_task_router_params = 0
            for param in self.ifs_router_A_list[task_id].parameters():
                current_task_router_params += param.numel()
            for param in self.ifs_router_B_list[task_id].parameters():
                current_task_router_params += param.numel()
                
            total_activated_params = current_task_lora_params + current_task_router_params
            
            # è®¡ç®—å…¶ä»–ä»»åŠ¡çš„å†»ç»“å‚æ•°æ•°é‡
            total_frozen_params = 0
            for i in range(self.num_tasks):
                if i != task_id:
                    frozen_lora_params = (
                        self.lora_A_pools[i].numel() + 
                        self.lora_B_pools[i].numel() +
                        self.cfs_router_B_weights_list[i].numel()
                    )
                    
                    frozen_router_params = 0
                    for param in self.ifs_router_A_list[i].parameters():
                        frozen_router_params += param.numel()
                    for param in self.ifs_router_B_list[i].parameters():
                        frozen_router_params += param.numel()
                        
                    total_frozen_params += frozen_lora_params + frozen_router_params
            
            logger.info(f"[ä»£è¡¨æ€§é€‚é…å™¨] ä»»åŠ¡åˆ‡æ¢: {old_task_id} -> {task_id}, "
                       f"æ¿€æ´»å‚æ•°: {total_activated_params}, å†»ç»“å‚æ•°: {total_frozen_params}")


    @torch.no_grad()
    def _update_task_key(self, query_signal: torch.Tensor):
        """ç¡®ä¿query_signalçš„ç»´åº¦å¤„ç†æ­£ç¡®"""
        try:
            batch_key = query_signal.mean(dim=0)
            old_key = self.task_keys[self.current_task_id]
            self.task_keys[self.current_task_id] = self.ema_momentum * old_key + (1 - self.ema_momentum) * batch_key
        except Exception as e:
            logger.error(f"Task keyæ›´æ–°é”™è¯¯: {e}")
            logger.error(f"query_signalå½¢çŠ¶: {query_signal.shape}")

    def forward(self, x: torch.Tensor, query_signal: torch.Tensor) -> torch.Tensor:
        task_id_to_use = self.current_task_id
        force_task_id = getattr(x, 'force_task_id', None)

        if not self.training:  # è¯„ä¼°æ¨¡å¼
            if force_task_id is not None:
                task_id_to_use = force_task_id
            else:  # è‡ªåŠ¨åŒ¹é…æœ€ä½³ä»»åŠ¡
                with torch.no_grad():
                    task_keys_on_device = self.task_keys.to(query_signal.device)
                    similarities = F.cosine_similarity(
                        query_signal.unsqueeze(1), 
                        task_keys_on_device.unsqueeze(0), 
                        dim=-1
                    )
                    best_task_ids = torch.argmax(similarities, dim=1)
                    task_id_to_use = torch.mode(best_task_ids).values.item()
        elif self.training:
            # ç¡®ä¿åœ¨è®­ç»ƒæ—¶ä½¿ç”¨æ­£ç¡®çš„ä»»åŠ¡ID
            assert task_id_to_use == self.current_task_id, \
                f"è®­ç»ƒæ—¶ä»»åŠ¡IDä¸åŒ¹é…: current={self.current_task_id}, using={task_id_to_use}"
            self._update_task_key(query_signal.detach())
        
        # é€‰æ‹©æ¿€æ´»çš„ç»„ä»¶
        active_A_pool = self.lora_A_pools[task_id_to_use]
        active_B_pool = self.lora_B_pools[task_id_to_use]
        active_ifs_router_A = self.ifs_router_A_list[task_id_to_use]
        active_ifs_router_B = self.ifs_router_B_list[task_id_to_use]
        active_cfs_router_B_weights = self.cfs_router_B_weights_list[task_id_to_use]

        # è·¯ç”±å’Œè®¡ç®—é€»è¾‘
        try:
            g_A_scores = active_ifs_router_A(query_signal)
            g_A_indices = torch.topk(g_A_scores, self.r, dim=-1).indices
            lora_A = active_A_pool[g_A_indices]

            g_B_scores_ifs = active_ifs_router_B(query_signal)
            g_B_scores_cfs = torch.einsum('bri,rie->be', lora_A.detach(), active_cfs_router_B_weights).sum(dim=1)
            
            g_B_scores = g_B_scores_ifs + g_B_scores_cfs
            g_B_indices = torch.topk(g_B_scores, self.r, dim=-1).indices
            lora_B = active_B_pool[g_B_indices]
            
            after_A = torch.einsum('bsi,bri->bsr', x, lora_A)
            lora_delta = torch.einsum('bsr,bor->bso', after_A, lora_B)
            
            return lora_delta * self.alpha
            
        except Exception as e:
            logger.error(f"MixLoRA forwardè®¡ç®—é”™è¯¯: {e}")
            logger.error(f"è¾“å…¥å½¢çŠ¶: x={x.shape}, query_signal={query_signal.shape}")
            logger.error(f"ä»»åŠ¡ID: {task_id_to_use}, å½“å‰ä»»åŠ¡: {self.current_task_id}")
            raise e