


# æ–‡ä»¶: src/model/rebq_mixlora.py (æœ€ç»ˆä¿®å¤ç‰ˆ)
import torch
import torch.nn as nn
from collections import OrderedDict

from .rebq_mixlora_vilt import RebQMixLoRAVilt
from loguru import logger
class RebQMixLoRA(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.backbone = RebQMixLoRAVilt(cfg)
        self.num_tasks = cfg.NUM_TASKS
        self.num_labels_per_task = cfg.NUM_LABELS_PER_TASK
        self.classifiers = nn.ModuleList([
            nn.Linear(cfg.HIDDEN_SIZE, self.num_labels_per_task) for _ in range(self.num_tasks)
        ])

        # åˆå§‹åŒ–å‚æ•°å†»ç»“
        for param in self.backbone.vilt.parameters():
            param.requires_grad = False
        
        # æ¿€æ´»Task 0çš„å‚æ•°å’Œåˆ†ç±»å¤´ï¼Œå†»ç»“å…¶ä»–çš„
        self.set_active_task(0) 
    
    def set_active_task(self, task_id: int):
        self.backbone.set_active_task(task_id)
        for i in range(self.num_tasks):
            is_active = (i == task_id)
            for param in self.classifiers[i].parameters():
                param.requires_grad = is_active



    def forward(self, batch, force_task_id: int = None):
        inputs = batch['inputs']
        
        # ğŸ”§ å¢å¼ºä¿®å¤ï¼šç¡®ä¿force_task_idè¢«æ­£ç¡®å¤„ç†
        outputs = self.backbone(
            input_ids=inputs.get("input_ids"),
            pixel_values=inputs.get("pixel_values"),
            attention_mask=inputs.get("attention_mask"),
            token_type_ids=inputs.get("token_type_ids"),
            pixel_mask=inputs.get("pixel_mask"),
            force_task_id=force_task_id
        )
        
        cls_token = outputs['pooler_output']
        
        # ğŸ”§ æ–°å¢ä¿®å¤ï¼šåœ¨è¯„ä¼°æ¨¡å¼ä¸‹ä¸”æœ‰force_task_idæ—¶ï¼Œåªä½¿ç”¨æŒ‡å®šä»»åŠ¡çš„åˆ†ç±»å¤´
        if not self.training and force_task_id is not None:
            # åªè®¡ç®—æŒ‡å®šä»»åŠ¡çš„logits
            task_logits = self.classifiers[force_task_id](cls_token)
            
            # æ„é€ å®Œæ•´çš„logitså¼ é‡ï¼Œå…¶ä»–ä»»åŠ¡çš„ä½ç½®å¡«é›¶
            full_logits = torch.zeros(
                cls_token.size(0), 
                self.num_tasks * self.num_labels_per_task, 
                device=cls_token.device
            )
            start_idx = force_task_id * self.num_labels_per_task
            end_idx = (force_task_id + 1) * self.num_labels_per_task
            full_logits[:, start_idx:end_idx] = task_logits
            
            return {"logits": full_logits, "labels": batch["labels"]}
        
        # è®­ç»ƒæ¨¡å¼æˆ–æ²¡æœ‰force_task_idæ—¶ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        all_logits = [self.classifiers[i](cls_token) for i in range(self.num_tasks)]
        logits = torch.cat(all_logits, dim=1)
        
        results = {"logits": logits, "labels": batch["labels"]}

        if "query_signals" in outputs:
            results["query_signals"] = outputs["query_signals"]
        
        results["missing_types"] = batch["missing_types"]
        complete_indices = (batch['missing_types'] == 0).nonzero(as_tuple=True)[0]
        results["complete_indices"] = complete_indices

        if self.training and len(complete_indices) > 0:
            # ä¸€è‡´æ€§æŸå¤±è®¡ç®—é€»è¾‘ä¿æŒä¸å˜
            consistency_inputs = {
                'input_ids': inputs['input_ids'][complete_indices],
                'pixel_values': inputs['pixel_values'][complete_indices],
                'attention_mask': inputs['attention_mask'][complete_indices],
                'pixel_mask': inputs.get('pixel_mask')[complete_indices] if inputs.get('pixel_mask') is not None else None,
            }
            
            pad_token_id, cls_token_id = 0, 101
            dummy_text_ids = torch.full_like(consistency_inputs['input_ids'], pad_token_id)
            dummy_text_ids[:, 0] = cls_token_id
            
            with torch.no_grad():
                consistency_outputs = self.backbone(
                    input_ids=dummy_text_ids,
                    pixel_values=consistency_inputs.get("pixel_values"),
                    attention_mask=consistency_inputs.get("attention_mask"),
                    token_type_ids=None,
                    pixel_mask=consistency_inputs.get("pixel_mask"),
                    force_task_id=self.backbone.vilt.encoder.layer[0].attention.attention.query.vision_adapter.current_task_id
                )

            consistency_cls = consistency_outputs['pooler_output']
            all_consistency_logits = [self.classifiers[i](consistency_cls) for i in range(self.num_tasks)]
            consistency_logits = torch.cat(all_consistency_logits, dim=1)
            results["consistency_logits"] = consistency_logits

        return results

    # ==================== æ ¸å¿ƒä¿®å¤ï¼šè‡ªå®šä¹‰çŠ¶æ€å­—å…¸é€»è¾‘ ====================
    def state_dict(self, destination=None, prefix='', keep_vars=False):
        """
        è‡ªå®šä¹‰state_dictï¼Œç¡®ä¿æ‰€æœ‰ä»»åŠ¡çš„ä¸“å®¶å’Œåˆ†ç±»å¤´éƒ½è¢«ä¿å­˜ã€‚
        """
        if destination is None:
            destination = OrderedDict()
        
        # 1. ä¿å­˜backboneä¸­æ‰€æœ‰æˆ‘ä»¬è‡ªå®šä¹‰çš„ã€éœ€è¦æŒç»­å­¦ä¹ çš„å‚æ•°
        #    æˆ‘ä»¬ç›´æ¥è®¿é—®åº•å±‚çš„ ParameterList å’Œ ModuleList
        for i in range(self.num_tasks):
            # ä¿å­˜LoRA Aå’ŒBçš„å› å­æ± 
            destination[prefix + f'backbone.vision_adapter.lora_A_pools.{i}'] = self.backbone.vilt.encoder.layer[0].attention.attention.query.vision_adapter.lora_A_pools[i]
            destination[prefix + f'backbone.vision_adapter.lora_B_pools.{i}'] = self.backbone.vilt.encoder.layer[0].attention.attention.query.vision_adapter.lora_B_pools[i]
            destination[prefix + f'backbone.text_adapter.lora_A_pools.{i}'] = self.backbone.vilt.encoder.layer[0].attention.attention.query.text_adapter.lora_A_pools[i]
            destination[prefix + f'backbone.text_adapter.lora_B_pools.{i}'] = self.backbone.vilt.encoder.layer[0].attention.attention.query.text_adapter.lora_B_pools[i]
            
            # ä¿å­˜è·¯ç”±å™¨çš„çŠ¶æ€
            destination.update(self.backbone.vilt.encoder.layer[0].attention.attention.query.vision_adapter.ifs_router_A_list[i].state_dict(prefix=prefix + f'backbone.vision_adapter.routers.{i}.ifs_A.'))
            # ... æ­¤å¤„éœ€è¦ä¸ºæ‰€æœ‰æ³¨å…¥LoRAçš„å±‚çš„ã€æ‰€æœ‰é€‚é…å™¨å’Œæ‰€æœ‰è·¯ç”±å™¨éƒ½æ·»åŠ ä¿å­˜é€»è¾‘ ...
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨æ•´ä¸ªæ¨¡å‹ï¼Œç„¶åç­›é€‰
            
        # æ›´ç®€å•ã€æ›´é²æ£’çš„æ–¹å¼æ˜¯è°ƒç”¨åŸå§‹çš„state_dictï¼Œå¹¶ç¡®ä¿å®ƒåŒ…å«äº†æ‰€æœ‰æˆ‘ä»¬æƒ³è¦çš„ä¸œè¥¿
        # PyTorchçš„ModuleListå’ŒParameterListæ˜¯èƒ½è¢«state_dict()æ­£ç¡®å¤„ç†çš„
        # è®©æˆ‘ä»¬é‡æ–°æ€è€ƒï¼Œä¸ºä»€ä¹ˆé»˜è®¤çš„state_dict()ä¼šå¤±è´¥ï¼Ÿ
        # å¤±è´¥çš„åŸå› å¯èƒ½ä¸æ˜¯å› ä¸ºrequires_grad=Falseï¼Œè€Œæ˜¯å› ä¸ºåŠ¨æ€æ›¿æ¢äº†æ¨¡å—ã€‚
        # æˆ‘ä»¬çš„æ³¨å…¥æ–¹å¼æ˜¯ self.query = MixLoRAWrapper(self.query, ...), è¿™å¯èƒ½ç ´åäº†åŸå§‹çš„æ¨¡å—æ³¨å†Œæ ‘ã€‚
        
        # è®©æˆ‘ä»¬é‡‡å–æœ€ä¿é™©çš„æ–¹å¼ï¼šåˆ†åˆ«è·å–backboneå’Œclassifiersçš„çŠ¶æ€å­—å…¸ï¼Œç„¶ååˆå¹¶
        # è¿™ä¸ªå‡è®¾æ˜¯ï¼šbackboneå’Œclassifierså†…éƒ¨çš„state_dict()èƒ½æ­£ç¡®å·¥ä½œã€‚
        
        # æ¸…ç†æ—§çš„ç®€å•å®ç°ï¼Œä½¿ç”¨PyTorchæ¨èçš„æ ‡å‡†æ–¹å¼
        super_state_dict = super().state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
        return super_state_dict

    def load_state_dict(self, state_dict, strict=True):
        """ğŸ”§ ä¿®å¤1: è‡ªå®šä¹‰çŠ¶æ€å­—å…¸åŠ è½½ï¼Œå¤„ç†é”®åä¸åŒ¹é…é—®é¢˜"""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§æ ¼å¼çš„checkpoint
        old_format_keys = [k for k in state_dict.keys() if k.startswith("backbone.vision_adapter.") or k.startswith("backbone.text_adapter.")]
        
        if old_format_keys:
            logger.warning("æ£€æµ‹åˆ°æ—§æ ¼å¼çš„checkpointï¼Œæ­£åœ¨è¿›è¡Œé”®åè½¬æ¢...")
            state_dict = self._convert_old_checkpoint(state_dict)
        
        # æ£€æŸ¥é”®ååŒ¹é…æƒ…å†µ
        model_keys = set(self.state_dict().keys())
        checkpoint_keys = set(state_dict.keys())
        
        missing_keys = model_keys - checkpoint_keys
        unexpected_keys = checkpoint_keys - model_keys
        
        if missing_keys:
            logger.warning(f"æ¨¡å‹ä¸­ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
            for key in list(missing_keys)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"  ç¼ºå¤±: {key}")
            if len(missing_keys) > 5:
                logger.warning(f"  ... è¿˜æœ‰ {len(missing_keys) - 5} ä¸ªç¼ºå¤±é”®")
        
        if unexpected_keys:
            logger.warning(f"checkpointä¸­å¤šä½™çš„é”®: {len(unexpected_keys)} ä¸ª")
            for key in list(unexpected_keys)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"  å¤šä½™: {key}")
            if len(unexpected_keys) > 5:
                logger.warning(f"  ... è¿˜æœ‰ {len(unexpected_keys) - 5} ä¸ªå¤šä½™é”®")
        
        # å°è¯•å®½æ¾åŠ è½½
        try:
            return super().load_state_dict(state_dict, strict=False)
        except Exception as e:
            logger.error(f"å³ä½¿ä½¿ç”¨strict=Falseä¹Ÿæ— æ³•åŠ è½½checkpoint: {e}")
            raise e

    def _convert_old_checkpoint(self, old_state_dict):
        """ğŸ”§ ä¿®å¤2: è½¬æ¢æ—§æ ¼å¼çš„checkpointé”®å"""
        new_state_dict = OrderedDict()
        
        # æ˜ å°„è§„åˆ™ï¼šæ—§é”®å -> æ–°é”®åçš„æ¨¡å¼
        conversion_patterns = [
            # æ—§æ ¼å¼: backbone.vision_adapter.lora_A_pools.X
            # æ–°æ ¼å¼: backbone.vilt.encoder.layer.Y.attention.attention.Z.vision_adapter.lora_A_pools.X
            ("backbone.vision_adapter.", "backbone.vilt.encoder.layer.0.attention.attention.query.vision_adapter."),
            ("backbone.text_adapter.", "backbone.vilt.encoder.layer.0.attention.attention.query.text_adapter."),
        ]
        
        for old_key, old_value in old_state_dict.items():
            new_key = old_key
            
            # åº”ç”¨è½¬æ¢è§„åˆ™
            for old_pattern, new_pattern in conversion_patterns:
                if old_key.startswith(old_pattern):
                    new_key = old_key.replace(old_pattern, new_pattern, 1)
                    break
            
            new_state_dict[new_key] = old_value
        
        logger.info(f"è½¬æ¢äº† {len([k for k in old_state_dict.keys() if any(k.startswith(p[0]) for p in conversion_patterns)])} ä¸ªæ—§æ ¼å¼é”®å")
        return new_state_dict